{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd482aa1",
   "metadata": {},
   "source": [
    "# Training\n",
    "Here we provide interactive scripts, with which one can train a generative model. The script uses classes and functions used in the code that runs on condor in order to obtain the optimised model trained on the full dataset. \n",
    "We advise not to train the model on the full dataset here as it will take too long. Limit the training dataset size (e.g. use a single input file) and number of epochs and use this notebook to study e.g pre-processing, hyperparameters and diffusion techniques.\n",
    "\n",
    "First, we set the environment and the various parameters for the model, hyperparameters, diffusion equations etc. We also create instances of the stochastic differential equations, machine learning model and create a list of files we want to use as input.\n",
    "\n",
    "Important: need to set the padding value to the value used in pad_events.py script to create the padded (and transformed) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988fc9fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Oct 26 16:35:25 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            On   | 00000000:00:07.0 Off |                    0 |\n",
      "| N/A   59C    P0    30W /  70W |  11533MiB / 15360MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n",
      "torch version:  1.13.0a0\n",
      "Running on device:  cuda\n",
      "Cuda used to build pyTorch:  11.8\n",
      "Current device:  0\n",
      "Cuda arch list:  ['sm_37', 'sm_50', 'sm_60', 'sm_61', 'sm_70', 'sm_75', 'compute_37']\n",
      "Working directory:  ./\n",
      "['/eos/user/j/jthomasw/tdsm_encoder/datasets/ds2_diff_transforms/dataset_2_padded_nentry424To564.pt']\n",
      "+--------------------------------+-------------------+\n",
      "|          Module name           | Parameters listed |\n",
      "+--------------------------------+-------------------+\n",
      "|           cls_token            |        512        |\n",
      "|          embed.weight          |        2048       |\n",
      "|           embed.bias           |        512        |\n",
      "|        embed_t.1.weight        |       262144      |\n",
      "|         embed_t.1.bias         |        512        |\n",
      "|      dense1.dense.weight       |        512        |\n",
      "|       dense1.dense.bias        |         1         |\n",
      "| encoder.0.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.0.attn.in_proj_bias   |        1536       |\n",
      "| encoder.0.attn.out_proj.weight |       262144      |\n",
      "|  encoder.0.attn.out_proj.bias  |        512        |\n",
      "|    encoder.0.ffnn.0.weight     |       65536       |\n",
      "|     encoder.0.ffnn.0.bias      |        128        |\n",
      "|    encoder.0.ffnn.3.weight     |       65536       |\n",
      "|     encoder.0.ffnn.3.bias      |        512        |\n",
      "|     encoder.0.norm1.weight     |        512        |\n",
      "|      encoder.0.norm1.bias      |        512        |\n",
      "|     encoder.0.norm2.weight     |        512        |\n",
      "|      encoder.0.norm2.bias      |        512        |\n",
      "| encoder.1.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.1.attn.in_proj_bias   |        1536       |\n",
      "| encoder.1.attn.out_proj.weight |       262144      |\n",
      "|  encoder.1.attn.out_proj.bias  |        512        |\n",
      "|    encoder.1.ffnn.0.weight     |       65536       |\n",
      "|     encoder.1.ffnn.0.bias      |        128        |\n",
      "|    encoder.1.ffnn.3.weight     |       65536       |\n",
      "|     encoder.1.ffnn.3.bias      |        512        |\n",
      "|     encoder.1.norm1.weight     |        512        |\n",
      "|      encoder.1.norm1.bias      |        512        |\n",
      "|     encoder.1.norm2.weight     |        512        |\n",
      "|      encoder.1.norm2.bias      |        512        |\n",
      "| encoder.2.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.2.attn.in_proj_bias   |        1536       |\n",
      "| encoder.2.attn.out_proj.weight |       262144      |\n",
      "|  encoder.2.attn.out_proj.bias  |        512        |\n",
      "|    encoder.2.ffnn.0.weight     |       65536       |\n",
      "|     encoder.2.ffnn.0.bias      |        128        |\n",
      "|    encoder.2.ffnn.3.weight     |       65536       |\n",
      "|     encoder.2.ffnn.3.bias      |        512        |\n",
      "|     encoder.2.norm1.weight     |        512        |\n",
      "|      encoder.2.norm1.bias      |        512        |\n",
      "|     encoder.2.norm2.weight     |        512        |\n",
      "|      encoder.2.norm2.bias      |        512        |\n",
      "| encoder.3.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.3.attn.in_proj_bias   |        1536       |\n",
      "| encoder.3.attn.out_proj.weight |       262144      |\n",
      "|  encoder.3.attn.out_proj.bias  |        512        |\n",
      "|    encoder.3.ffnn.0.weight     |       65536       |\n",
      "|     encoder.3.ffnn.0.bias      |        128        |\n",
      "|    encoder.3.ffnn.3.weight     |       65536       |\n",
      "|     encoder.3.ffnn.3.bias      |        512        |\n",
      "|     encoder.3.norm1.weight     |        512        |\n",
      "|      encoder.3.norm1.bias      |        512        |\n",
      "|     encoder.3.norm2.weight     |        512        |\n",
      "|      encoder.3.norm2.bias      |        512        |\n",
      "| encoder.4.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.4.attn.in_proj_bias   |        1536       |\n",
      "| encoder.4.attn.out_proj.weight |       262144      |\n",
      "|  encoder.4.attn.out_proj.bias  |        512        |\n",
      "|    encoder.4.ffnn.0.weight     |       65536       |\n",
      "|     encoder.4.ffnn.0.bias      |        128        |\n",
      "|    encoder.4.ffnn.3.weight     |       65536       |\n",
      "|     encoder.4.ffnn.3.bias      |        512        |\n",
      "|     encoder.4.norm1.weight     |        512        |\n",
      "|      encoder.4.norm1.bias      |        512        |\n",
      "|     encoder.4.norm2.weight     |        512        |\n",
      "|      encoder.4.norm2.bias      |        512        |\n",
      "| encoder.5.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.5.attn.in_proj_bias   |        1536       |\n",
      "| encoder.5.attn.out_proj.weight |       262144      |\n",
      "|  encoder.5.attn.out_proj.bias  |        512        |\n",
      "|    encoder.5.ffnn.0.weight     |       65536       |\n",
      "|     encoder.5.ffnn.0.bias      |        128        |\n",
      "|    encoder.5.ffnn.3.weight     |       65536       |\n",
      "|     encoder.5.ffnn.3.bias      |        512        |\n",
      "|     encoder.5.norm1.weight     |        512        |\n",
      "|      encoder.5.norm1.bias      |        512        |\n",
      "|     encoder.5.norm2.weight     |        512        |\n",
      "|      encoder.5.norm2.bias      |        512        |\n",
      "| encoder.6.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.6.attn.in_proj_bias   |        1536       |\n",
      "| encoder.6.attn.out_proj.weight |       262144      |\n",
      "|  encoder.6.attn.out_proj.bias  |        512        |\n",
      "|    encoder.6.ffnn.0.weight     |       65536       |\n",
      "|     encoder.6.ffnn.0.bias      |        128        |\n",
      "|    encoder.6.ffnn.3.weight     |       65536       |\n",
      "|     encoder.6.ffnn.3.bias      |        512        |\n",
      "|     encoder.6.norm1.weight     |        512        |\n",
      "|      encoder.6.norm1.bias      |        512        |\n",
      "|     encoder.6.norm2.weight     |        512        |\n",
      "|      encoder.6.norm2.bias      |        512        |\n",
      "| encoder.7.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.7.attn.in_proj_bias   |        1536       |\n",
      "| encoder.7.attn.out_proj.weight |       262144      |\n",
      "|  encoder.7.attn.out_proj.bias  |        512        |\n",
      "|    encoder.7.ffnn.0.weight     |       65536       |\n",
      "|     encoder.7.ffnn.0.bias      |        128        |\n",
      "|    encoder.7.ffnn.3.weight     |       65536       |\n",
      "|     encoder.7.ffnn.3.bias      |        512        |\n",
      "|     encoder.7.norm1.weight     |        512        |\n",
      "|      encoder.7.norm1.bias      |        512        |\n",
      "|     encoder.7.norm2.weight     |        512        |\n",
      "|      encoder.7.norm2.bias      |        512        |\n",
      "|           out.weight           |        2048       |\n",
      "|            out.bias            |         4         |\n",
      "+--------------------------------+-------------------+\n",
      "Sum of trainable parameters: 9743365\n"
     ]
    }
   ],
   "source": [
    "import time, functools, torch, os,sys, random, fnmatch, psutil\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam,RAdam\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from prettytable import PrettyTable\n",
    "import tqdm\n",
    "from pickle import load\n",
    "from IPython import display\n",
    "\n",
    "sys.path.insert(1, '../')\n",
    "#import trans_tdsm, utils\n",
    "import util.data_utils as utils\n",
    "import util.score_model as score_model\n",
    "import util.sdes as sdes\n",
    "import util.display\n",
    "\n",
    "# GPU device info\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:516\"\n",
    "os.system('nvidia-smi')\n",
    "\n",
    "# Set padding value used\n",
    "padding_value = 0.0\n",
    "\n",
    "dataset = \"dataset_2_padded_nentry\"\n",
    "preproc_dataset_name = 'ds2_diff_transforms'\n",
    "dataset_store_path = os.path.join(\"/eos/user/j/jthomasw/tdsm_encoder/datasets/\", preproc_dataset_name)\n",
    "transform = None\n",
    "transform_y = None\n",
    "mask = True\n",
    "jupyternotebook = True\n",
    "workingdir = \"./\"\n",
    "\n",
    "### SDE PARAMETERS ###\n",
    "SDE = 'VP'\n",
    "if SDE == 'VP':\n",
    "    beta_max = 5.0\n",
    "    beta_min = 0.001\n",
    "if SDE == 'VE':\n",
    "    sigma_max = 20.0\n",
    "    sigma_min = 0.1\n",
    "    \n",
    "### MODEL PARAMETERS ###\n",
    "n_feat_dim = 4\n",
    "embed_dim = 512\n",
    "hidden_dim = 128\n",
    "num_encoder_blocks = 8\n",
    "num_attn_heads = 16\n",
    "dropout_gen = 0\n",
    "\n",
    "# Instantiate stochastic differential equation\n",
    "if SDE == 'VP':\n",
    "    sde = sdes.VPSDE(beta_max=beta_max,beta_min=beta_min, device=device)\n",
    "if SDE == 'VE':\n",
    "    sde = sdes.VESDE(sigma_max=sigma_max,sigma_min=sigma_min,device=device)\n",
    "marginal_prob_std_fn = functools.partial(sde.marginal_prob)\n",
    "diffusion_coeff_fn = functools.partial(sde.sde)\n",
    "\n",
    "print('torch version: ', torch.__version__)\n",
    "print('Running on device: ', device)\n",
    "if torch.cuda.is_available():\n",
    "    print('Cuda used to build pyTorch: ',torch.version.cuda)\n",
    "    print('Current device: ', torch.cuda.current_device())\n",
    "    print('Cuda arch list: ', torch.cuda.get_arch_list())\n",
    "\n",
    "print('Working directory: ', workingdir)\n",
    "\n",
    "# Input files\n",
    "files_list_ = []\n",
    "for filename in os.listdir(dataset_store_path):\n",
    "    if fnmatch.fnmatch(filename, dataset + '*424To564.pt'):\n",
    "        files_list_.append(os.path.join(dataset_store_path, filename))\n",
    "print(files_list_)\n",
    "\n",
    "# Instantiate model\n",
    "model=score_model.Gen(n_feat_dim, embed_dim, hidden_dim, num_encoder_blocks, num_attn_heads, dropout_gen, marginal_prob_std=marginal_prob_std_fn)\n",
    "torch.save(model.state_dict(), 'initial_model.pt')\n",
    "table = PrettyTable(['Module name', 'Parameters listed'])\n",
    "t_params = 0\n",
    "for name_ , para_ in model.named_parameters():\n",
    "    if not para_.requires_grad: continue\n",
    "    param = para_.numel()\n",
    "    table.add_row([name_, param])\n",
    "    t_params+=param\n",
    "print(table)\n",
    "print(f'Sum of trainable parameters: {t_params}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fe119",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The following cell will call training methods from the trans_tdsm.py script so everything should be synchronised with what we can run on condor for the big jobs.\n",
    "\n",
    "Once you have a a fully trained model you can look at the sampling notebook to see how to generate samples and make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3b437",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00624aa41d6b458abe1f02c2b6bae57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-------------------+\n",
      "|          Module name           | Parameters listed |\n",
      "+--------------------------------+-------------------+\n",
      "|           cls_token            |        512        |\n",
      "|          embed.weight          |        2048       |\n",
      "|           embed.bias           |        512        |\n",
      "|        embed_t.1.weight        |       262144      |\n",
      "|         embed_t.1.bias         |        512        |\n",
      "|      dense1.dense.weight       |        512        |\n",
      "|       dense1.dense.bias        |         1         |\n",
      "| encoder.0.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.0.attn.in_proj_bias   |        1536       |\n",
      "| encoder.0.attn.out_proj.weight |       262144      |\n",
      "|  encoder.0.attn.out_proj.bias  |        512        |\n",
      "|    encoder.0.ffnn.0.weight     |       65536       |\n",
      "|     encoder.0.ffnn.0.bias      |        128        |\n",
      "|    encoder.0.ffnn.3.weight     |       65536       |\n",
      "|     encoder.0.ffnn.3.bias      |        512        |\n",
      "|     encoder.0.norm1.weight     |        512        |\n",
      "|      encoder.0.norm1.bias      |        512        |\n",
      "|     encoder.0.norm2.weight     |        512        |\n",
      "|      encoder.0.norm2.bias      |        512        |\n",
      "| encoder.1.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.1.attn.in_proj_bias   |        1536       |\n",
      "| encoder.1.attn.out_proj.weight |       262144      |\n",
      "|  encoder.1.attn.out_proj.bias  |        512        |\n",
      "|    encoder.1.ffnn.0.weight     |       65536       |\n",
      "|     encoder.1.ffnn.0.bias      |        128        |\n",
      "|    encoder.1.ffnn.3.weight     |       65536       |\n",
      "|     encoder.1.ffnn.3.bias      |        512        |\n",
      "|     encoder.1.norm1.weight     |        512        |\n",
      "|      encoder.1.norm1.bias      |        512        |\n",
      "|     encoder.1.norm2.weight     |        512        |\n",
      "|      encoder.1.norm2.bias      |        512        |\n",
      "| encoder.2.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.2.attn.in_proj_bias   |        1536       |\n",
      "| encoder.2.attn.out_proj.weight |       262144      |\n",
      "|  encoder.2.attn.out_proj.bias  |        512        |\n",
      "|    encoder.2.ffnn.0.weight     |       65536       |\n",
      "|     encoder.2.ffnn.0.bias      |        128        |\n",
      "|    encoder.2.ffnn.3.weight     |       65536       |\n",
      "|     encoder.2.ffnn.3.bias      |        512        |\n",
      "|     encoder.2.norm1.weight     |        512        |\n",
      "|      encoder.2.norm1.bias      |        512        |\n",
      "|     encoder.2.norm2.weight     |        512        |\n",
      "|      encoder.2.norm2.bias      |        512        |\n",
      "| encoder.3.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.3.attn.in_proj_bias   |        1536       |\n",
      "| encoder.3.attn.out_proj.weight |       262144      |\n",
      "|  encoder.3.attn.out_proj.bias  |        512        |\n",
      "|    encoder.3.ffnn.0.weight     |       65536       |\n",
      "|     encoder.3.ffnn.0.bias      |        128        |\n",
      "|    encoder.3.ffnn.3.weight     |       65536       |\n",
      "|     encoder.3.ffnn.3.bias      |        512        |\n",
      "|     encoder.3.norm1.weight     |        512        |\n",
      "|      encoder.3.norm1.bias      |        512        |\n",
      "|     encoder.3.norm2.weight     |        512        |\n",
      "|      encoder.3.norm2.bias      |        512        |\n",
      "| encoder.4.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.4.attn.in_proj_bias   |        1536       |\n",
      "| encoder.4.attn.out_proj.weight |       262144      |\n",
      "|  encoder.4.attn.out_proj.bias  |        512        |\n",
      "|    encoder.4.ffnn.0.weight     |       65536       |\n",
      "|     encoder.4.ffnn.0.bias      |        128        |\n",
      "|    encoder.4.ffnn.3.weight     |       65536       |\n",
      "|     encoder.4.ffnn.3.bias      |        512        |\n",
      "|     encoder.4.norm1.weight     |        512        |\n",
      "|      encoder.4.norm1.bias      |        512        |\n",
      "|     encoder.4.norm2.weight     |        512        |\n",
      "|      encoder.4.norm2.bias      |        512        |\n",
      "| encoder.5.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.5.attn.in_proj_bias   |        1536       |\n",
      "| encoder.5.attn.out_proj.weight |       262144      |\n",
      "|  encoder.5.attn.out_proj.bias  |        512        |\n",
      "|    encoder.5.ffnn.0.weight     |       65536       |\n",
      "|     encoder.5.ffnn.0.bias      |        128        |\n",
      "|    encoder.5.ffnn.3.weight     |       65536       |\n",
      "|     encoder.5.ffnn.3.bias      |        512        |\n",
      "|     encoder.5.norm1.weight     |        512        |\n",
      "|      encoder.5.norm1.bias      |        512        |\n",
      "|     encoder.5.norm2.weight     |        512        |\n",
      "|      encoder.5.norm2.bias      |        512        |\n",
      "| encoder.6.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.6.attn.in_proj_bias   |        1536       |\n",
      "| encoder.6.attn.out_proj.weight |       262144      |\n",
      "|  encoder.6.attn.out_proj.bias  |        512        |\n",
      "|    encoder.6.ffnn.0.weight     |       65536       |\n",
      "|     encoder.6.ffnn.0.bias      |        128        |\n",
      "|    encoder.6.ffnn.3.weight     |       65536       |\n",
      "|     encoder.6.ffnn.3.bias      |        512        |\n",
      "|     encoder.6.norm1.weight     |        512        |\n",
      "|      encoder.6.norm1.bias      |        512        |\n",
      "|     encoder.6.norm2.weight     |        512        |\n",
      "|      encoder.6.norm2.bias      |        512        |\n",
      "| encoder.7.attn.in_proj_weight  |       786432      |\n",
      "|  encoder.7.attn.in_proj_bias   |        1536       |\n",
      "| encoder.7.attn.out_proj.weight |       262144      |\n",
      "|  encoder.7.attn.out_proj.bias  |        512        |\n",
      "|    encoder.7.ffnn.0.weight     |       65536       |\n",
      "|     encoder.7.ffnn.0.bias      |        128        |\n",
      "|    encoder.7.ffnn.3.weight     |       65536       |\n",
      "|     encoder.7.ffnn.3.bias      |        512        |\n",
      "|     encoder.7.norm1.weight     |        512        |\n",
      "|      encoder.7.norm1.bias      |        512        |\n",
      "|     encoder.7.norm2.weight     |        512        |\n",
      "|      encoder.7.norm2.bias      |        512        |\n",
      "|           out.weight           |        2048       |\n",
      "|            out.bias            |         4         |\n",
      "+--------------------------------+-------------------+\n",
      "Sum of trainable parameters: 9743365\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhcAAAERCAYAAADIaIXIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAyrUlEQVR4nO3deZwU1bn/8c8zCwwyowi4sBgWlwRBBMFdEzBi0BgRE5S4Rgkqr+uW+wtGchNxy9Vr1Cgx0SsRNQkRUUMgikuMO3FjM6LoBRV1BGWTfZ3h+f1xupmF7lmru3pmvu/Xq15VXX361DM11PD0qVPnmLsjIiIiEpW8uAMQERGR5kXJhYiIiERKyYWIiIhESsmFiIiIRErJhYiIiERKyYWIiIhEqiDuAHJVx44dvXv37nGHIZLz5syZs9Ld94o7jnR0LYvUTZTXspKLNLp3787s2bPjDkMk55nZJ3HHUBNdyyJ1E+W1rNsiIiIiEiklFyIiIhIpJRciIiISKfW5kBZv+/btlJaWsmXLlrhDyWlFRUV07dqVwsLCuEMRkRyn5EJavNLSUkpKSujevTtmFnc4OcndWbVqFaWlpfTo0SPucEQkx+m2iLR4W7ZsoUOHDkosamBmdOjQQa07IlInSi5EQIlFHegciUhdKbloILOwiDTWmjVr+P3vf1/vz51yyimsWbOmxjLXXnstzz33XAMjE5GWIBP/nym5EIlZuuSivLy8xs/NnDmTdu3a1Vjmhhtu4MQTT2xMeCIi9abkQiRm11xzDR9++CH9+vXj8MMPZ/DgwZx99tkccsghAJx++ukMGDCA3r17c9999+38XPfu3Vm5ciVLliyhV69ejB49mt69e3PSSSexefNmAH70ox/x2GOP7Sw/fvx4DjvsMA455BDef/99AFasWMGQIUM47LDDuOSSS+jWrRsrV67M8lkQkeZEyUUjbdgQdwTS1N1yyy3sv//+zJ8/n1//+te8+eab/OpXv+K9994DYNKkScyZM4fZs2czYcIEVq1atUsdixYt4j/+4z949913adeuHY8//njKY3Xs2JG5c+cyZswYbrvtNgCuv/56TjjhBObOncvw4cP59NNPM/fDikiLoEdRG+nnP4cJE+KOQqJy1VUwf360dfbrB3feWffyRxxxRJXHPSdMmMC0adMA+Oyzz1i0aBEdOnSo8pkePXrQr18/AAYMGMCSJUtS1n3GGWfsLPPXv/4VgFdffXVn/UOHDmXPPfese7AiIimo5aKREn+fRSLTtm3bndsvvvgizz33HK+99hpvv/02/fv3T/k4aOvWrXdu5+fnU1ZWlrLuZLnKZdw9yvBFRNRy0VCtWsG2bfDll3FHIlGqTwtDVEpKSli/fn3K99auXcuee+7Jbrvtxvvvv8/rr78e+fGPO+44pk6dys9+9jOeffZZvvrqq8iPISIti1ouGqh797BO8wVRpM46dOjAscceS58+fRg7dmyV94YOHUpZWRl9+/bll7/8JUcddVTkxx8/fjzPPvsshx12GE899RSdOnWipKQk8uOISMthahJNbeDAgT579uy07//2t3DFFWFbp7BpW7hwIb169Yo7jNhs3bqV/Px8CgoKeO211xgzZgzz03Q8SXWuzGyOuw/MQqgNUtu1LNLSVYxxEd21rNsiDXT55RXJhUhT9umnn3LmmWeyY8cOWrVqxcSJE+MOSUSaOCUXIi3cgQceyLx58+IOQ0SaEfW5EBERkUgpuRAREZFItajkwsx6mtn9ZvZYlPVqlE6R6GXqehWRzMt4cmFm+WY2z8yeaEQdk8xsuZktSPHeUDP7wMwWm9k1NdXj7h+5+6iGxpHOlVdGXaNIPMrLy+nfvz+nnnpqg+vI9etVRDIvGy0XVwILU71hZnubWUm1fQekKPogMDTF5/OB3wEnAwcDPzSzg83sEDN7otqyd2N/kHSefDJTNUtL0NAp1wHuvPNONm3atPN1XaZhr8ldd92V9rHc5cuXs3Hjxir7Fi9enKrog+Tw9SoimZfR5MLMugLfBf6Qpsi3gOlmVpQoPxrYZaYOd38ZWJ3i80cAixPfcLYBU4Bh7v6Ou59abVkexc9UWZs2YZ1iHimROosyuajLNOzplJaW8uSTT/LjH/845fsvvfQSl1122c7hxydOnMgVKZ7HztXrVUSyJ9MtF3cCVwM7Ur3p7o8CTwNTzOwc4CLgzHrU3wX4rNLr0sS+lMysg5ndC/Q3s3FpynzPzO5bu3ZtrQfv2TOsNUqnNEblKdfHjh3Lr3/9aw4//HD69u3L+PHjAdi4cSPf/e53OfTQQ+nTpw+PPPIIEyZMYOnSpQwePJjBgwcDdZuG/a233qJv374cffTRjB07lj59+gBw1VVXceutt5KXl/rPwogRIzjuuOMYOXIkkydPZtKkSUydOrU+P2qk16uZXWdmXtuydOnS+sQoIhHIWHJhZqcCy919Tk3l3P1WYAtwD3Cau9ene6Sl2Jd2vEx3X+Xul7r7/u5+c5oyf3f3i/fYY49aD3711XUPVCSdylOuDxkyhEWLFvHmm28yf/585syZw8svv8zTTz9N586defvtt1mwYAFDhw7liiuuoHPnzrzwwgu88MILu9Sbbhr2Cy+8kHvvvZfXXnuN/Px8AJ544gn23ntvBgwYUGOso0aNoqioiDFjxjBjxgyKi4vr86NGer26+3XubrUtnTt3rk+MIhKBTA6idSxwmpmdAhQBu5vZn9393MqFzOx4oA8wDRgPXFaPY5QC+1V63RXI2teU88+HCy7I1tEkK2Kec/3ZZ5/l2WefpX///gBs2LCBRYsWcfzxx/PTn/6Un/3sZ5x66qkcf/zxtdaVahr2NWvWsH79eo455hgAzj77bJ544glmzZrFjBkzmDlzJlu2bGHdunWce+65/PnPf65S5+zZs1mwYAHDhw/n+uuv5+67767zaSDm61VEsidjLRfuPs7du7p7d2Ak8HyKxKI/MBEYBlwItDezm+pxmLeAA82sh5m1ShxnRiQ/gEgM3J1x48Yxf/585s+fz+LFixk1ahQHHXQQc+bM4ZBDDmHcuHHccMMNtdaVahr2dHMJ3XzzzZSWlrJkyRKmTJnCCSecsEtiMW/ePMaPH8/06dN54IEHWL16Nb/4xS/q8+PpehVpIeIe/ns3YIS7fwhgZhcAP6peyMweBgYBHc2sFBjv7ve7e5mZXQY8A+QDk9z93WwFL81QDHOuV55y/Tvf+Q6//OUvOeeccyguLubzzz+nsLCQsrIy2rdvz7nnnktxcTEPPvhglc927NixTsfac889KSkp4fXXX+eoo45iypQpdY5z06ZN/OY3v2H//fcH4KGHHtoZR2W6XkUkK8mFu78IvJhi/6xqr7cTWjKql/thDXXPBGY2OkiRmFSecv3kk0/m7LPP5uijjwaguLiYP//5zyxevJixY8eSl5dHYWEh99xzDwAXX3wxJ598Mp06dUrZ7yKV+++/n9GjR9O2bVsGDRpE9f5FgwYNYtCgQbt87thjj2XhwoqnygsLCxk9ejQXX3xxlXK6XkVEU66nUddpmpNT1S5bBvvum+GgJCNa2pTrGzZs2NkR85ZbbmHZsmXcdddddfqsplwXaX4yMeV6ixr+O5OuuiruCETq5sknn6Rfv3706dOHV155pb79JkREahV3n4smzwzc4fnn445EpG7OOusszjrrrLjDEJFmTC0XjZQcpXN1qvEIRUREWiAlF4100EFhXV4ebxzSOOp7VDudIxGpKyUXjXT99XFHII1VVFTEqlWr9J9nDdydVatWUVRUFHcoItIEqM9FI512WtwRSGN17dqV0tJSVqxYEXcoOa2oqIiuXbvGHYaINAFKLqTFKywspEePHnGHISLSbOi2iIiIiERKyYWIiIhESslFhL74Iu4IRERE4qfkIkKXXhp3BCIiIvFTchGB5Ljsr74abxwiIiK5QMlFBJKjdH71VbxxiIiI5AIlFxHo0yesd+yINw4REZFcoOQiAr/6VdwRiIiI5A4lFxE48cS4IxAREckdSi5EREQkUkouREREJFJKLkRERCRSSi4itnhx3BGIiIjES8lFxC67LO4IRERE4qXkIiLJUTrfeCPeOEREROKm5CIixcVhvW5dvHGIiIjETclFRA49NKw1SqeIiLR0Si4icsstcUcgIiKSG5RcROTYY+OOQEREJDcouRAREZFIKbkQERGRSCm5EBERkUi1qOTCzHqa2f1m9lgmjzN/fiZrF2kZsnW9ikj0MpZcmFmRmb1pZm+b2btmdn0j6ppkZsvNbEGK94aa2QdmttjMrqmpHnf/yN1HNTSOurrqqkwfQSRaW7Zs4YgjjuDQQw+ld+/ejB8/vsF1NbXrVUSiV5DBurcCJ7j7BjMrBF41s6fc/fVkATPbG9js7usr7TvA3avP0PEgcDfwx8o7zSwf+B0wBCgF3jKzGUA+cHO1Oi5y9+XR/Gip5eWFcS7UciFNTevWrXn++ecpLi5m+/btHHfccZx88skcddRRO8ssX76cNm3aUFJSsnPf4tST6TxIE7heRQQ2bMhMvRlrufAgGXZhYvFqxb4FTDezIgAzGw1MSFHXy8DqFIc5Alic+IazDZgCDHP3d9z91GpLnf5Qmdn3zOy+tWvX1unnrCz5N3f9+prLieQaM6M4Mczs9u3b2b59O5Yc0z7hpZdeYtiwYWzZsgWAiRMncsUVV+xSV7auVzO7zsy8tmXp0qX1PBsiLUeTSy4gfFMxs/nAcuAf7l5l5g13fxR4GphiZucAFwFn1uMQXYDPKr0uTexLF08HM7sX6G9m41KVcfe/u/vFe+yxRz3CCAYODGuN0ilNUXl5Of369WPvvfdmyJAhHHnkkVXeHzFiBEOHDmXkyJFMnjyZSZMmMXXq1PocItLr1d2vc3erbencuXN9YhRpUTL1ZTijyYW7l7t7P6ArcISZ9UlR5lZgC3APcFql1o66sBT7qreOVD7WKne/1N33d/fqzbCNdtttUdcokj35+fnMnz+f0tJS3nzzTRYs2KXLBFdffTVFRUWMGTOGGTNm7GztqKOcul5FBJZn6OZjVp4Wcfc1wIvA0OrvmdnxQB9gGlDfXmSlwH6VXncFYmsD7dcvriOLRKddu3YMGjSIp59+epf3XnnlFRYsWMDw4cO5/vp699HOqetVRGDVqszUm8mnRfYys3aJ7TbAicD71cr0ByYCw4ALgfZmdlM9DvMWcKCZ9TCzVsBIYEYE4Yu0KCtWrGDNmjUAbN68meeee45vfOMbVcrMmzeP0aNHM336dB544AFWr17NL37xi/ocRterSI5JdKGKXCZbLjoBL5jZvwl/VP7h7k9UK7MbMMLdP3T3HcAFwCfVKzKzh4HXgK+bWamZjQJw9zLgMuAZYCEw1d3fzdhPJNJMLVu2jMGDB9O3b18OP/xwhgwZwqmnnlqlzKZNm3j00UfZf//9ycvL46GHHqJbt2671KXrVaTpSHyniJy5p73l2aINHDjQZ8+eXe/PJTvY67RKS2Fmc9x9YNxxpNPQa1mkJbjtNhg7Nvkqumu5RY3QmU2zZsUdgYiISM3WrctMvUouMuTqq+OOQEREpGbbtmWmXiUXEctLnNF33ok3DhERkdps2pSZepVcRGz33cM6U6OeiYiIRGXjxszUq+QiYsceG9bq0CkiIrlu69bM1KvkImJ33hl3BCIiInXTFMe5aJEOOCDuCEREROpGyYWIiIhEavv2zNSr5EJERKSFUp8LERERiZTGuWiC/v73uCMQERFJTy0XTdB118UdgYiISHo7dmSmXiUXGZAcpfP992suJyIiEid16GxC9twzrDdvjjcOERGRmii5aEIGDQprjdIpIiK5rLw8M/UquciAu++OOwIREZHaKbloQvbdN+4IREREaqfkQkRERCKlp0VEREQkUmVlYW0Wbb1KLkRERFootVw0UY8+GncEIiIiqSm5aKJuuinuCERERFJTctHE5OeH9aJF8cYhIiKSTnI8JvW5aCLatw/rLVvijUNERCQdtVw0MSedFNYapVNERHJVpv6PUnKRIffeG3cEIiIiNUsOoqXbIk1EcXHcEYiIiMRDyYWIiEgLFWuHTjNra2Z5ie2DzOw0MyuMNhQRERHJpmSHzrhui7wMFJlZF+CfwIXAg9GGIiIiItkUd4dOc/dNwBnAb919OHBwZkJqfiZOjDsCERGRXcWeXJjZ0cA5wJOJfQWZCan5ueOOuCMQERFJL67bIlcB44Bp7v6umfUEXog2lOanIJF+LVkSaxgiIiIpJVsukqNKR6VOrQ/u/hLwEkCiY+dKd78i2lCanw4d4MsvNUqniIi0LHV9WuQvZra7mbUF3gM+MLOxmQ2t6Rs2LO4IRERE0ot7bpGD3X0dcDowE/gacF60oTQ/t98edwQiIiLpJZOLvIhHvaprdYWJcS1OB6a7+3ZAs2bUQqN0iohIUxBXy8X/AkuAtsDLZtYNWBdtKCIiIhKHqFsu6tqhcwIwodKuT8xscLShiIiISBziGv57DzO7w8xmJ5bbCa0YIiIi0kRl6lHUujaETALWA2cmlnXAA9GG0rypc6eIiOSquPpc7O/u4939o8RyPdAz2lCat3vuiTsCERGR1OJqudhsZsclX5jZscDmaENpnpKjdH72WbxxiIiIpBPLCJ3ApcAfzWyPxOuvgAuiDaV52mcf+Pxz2LYt7khERESqinUQLXd/290PBfoCfd29P3BCtKE0T2eeGXcEIiIiNSuIeCrSej3Z6u7rEiN1AvxntKFknpn1NLP7zeyxbB3zhhuydSSR5iWO61WkpYqrz0UqNTaimNl+ZvaCmS00s3fN7MoGH8hskpktN7MFKd4bamYfmNliM7umpnoSnVFHNTSOhtAondIUfPbZZwwePJhevXrRu3dv7rrrrgbX1ZSvV5GWKurbIo1pCKlt+O8y4P+5+1wzKwHmmNk/3P29ZAEz2xvY7O7rK+07wN0XV6vrQeBu4I+Vd5pZPvA7YAhQCrxlZjOAfODmanVc5O7L6/zTibQgBQUF3H777Rx22GGsX7+eAQMGMGTIEA4++OCdZZYvX06bNm0oKSnZuW/x4uqXKqDrVaTJKSyMtr4akwszW0/qJMKANjV91t2XAcsS2+vNbCHQhTCratK3gDFmdoq7bzGz0cBw4JRqdb1sZt1THOYIYLG7f5SIdwowzN1vBk6tKT4RqdCpUyc6deoEQElJCb169eLzzz+vkly89NJL3HPPPcycOZOioiImTpzItGnTdqlL16tI09OqVbT11XhbxN1L3H33FEuJu9e51SPxh6Y/8Ea1+h8FngammNk5wEWEQbrqqgtQ+SHP0sS+dHF0MLN7gf5mNi5Nme+Z2X1r166tRxgizceSJUuYN28eRx55ZJX9I0aMYOjQoYwcOZLJkyczadIkpk6dWp+qI71ezew6M/PalqVLl9YnRpEWKdYOnQ1hZsXA48BVlTqD7uTutwJbgHuA09x9Q32qT7Ev7e0ad1/l7pe6+/6Jb0upyvzd3S/eY489Ur3dKL/6VeRVikRqw4YNfP/73+fOO+9k99133+X9q6++mqKiIsaMGcOMGTMorl+nokivV3e/zt2ttqVz5871iVGkRWpSyUVimvbHgcnu/tc0ZY4H+gDTgPH1PEQpsF+l112BnP2a8oc/xB2BSHrbt2/n+9//Pueccw5nnHFGyjKvvPIKCxYsYPjw4Vx//fX1PUSTul5FWpKs3hZpDDMz4H5gobvfkaZMf2AiMAy4EGhvZjfV4zBvAQeaWQ8zawWMBGY0LvLoJTvKfP55vHGIpOPujBo1il69evGf/5n6KfN58+YxevRopk+fzgMPPMDq1av5xS9+UZ/DNInrVaQlat062voy2XJxLHAecIKZzU8sp1Qrsxswwt0/dPcdhFE/P6lekZk9DLwGfN3MSs1sFIC7lwGXAc8AC4Gp7v5u5n6khkm2ym7fHm8cIunMmjWLP/3pTzz//PP069ePfv36MXPmzCplNm3axKOPPsr+++9PXl4eDz30EN26ddulrqZ+vYq0RFE/LWLutT1R2jINHDjQZ8+eHUld114LN94YtnW6pbkxsznuPjDuONKJ8loWaW6S41sMGwbTp0d3LWe8Q6fA1VfHHYGIiEh6RUXR1qfkIgs0SqeIiOSytm2jrU/JhYiISAsXdZ8LJRcNNWMGjBwZdxQiIiKNtttu0dan5KIhvvgi9H555JF6f3RDfYYIExERyYKob98ruWiIffet2K5nW9JN9RnFQ0REJAua0jgXzdv06WFdVgazZtX5Y3/5S4biERERaaCoZ7xQctFQp50GeYnTd9xxtRZPNnB88UUGYxIREWmAFFMJNYqSi8aoPHNqLZ0790vMqKBROkVEJNe0aRNtfUouGqO4GLokZoyupXPnRRdlIR4REZEGaN8+2vqUXDRWaWnFdseOaYv9139lIRYREZEG6NAh2vqUXEThJz8J61Wr9KypiIg0OZUfgoyCkoso3FFpRvmSkvjiEBERaQCNc5GrFi2q2L7vvvjiEBERqSclF7nqgAMqRiG55JIai+rOiYiINGdKLqK0ZUvF9oABaYv9/OdZiEVERCQmSi6idvTRYT13btoijz+epVhERERioOQiav/6V8V2tcHaW7UK6+XLsxiPiIhIlim5yISHHgrrbduqdPTs3j2sy8qyH5KIiEi2KLnIhPPPB7OwfdBBO3dfdllM8YiIiGSRkotMWbeuYvvyyyuvREREmjUlF5lSXAx77RW277473lhERESySMlFJlXuudm5c3xxiIiIZJGSi0xLToe6bJlGzxIRkRZByUWm3X9/xfbuu+/cVJ4hIiLNlZKLbJg3L6zdaccqAK68MsZ4REREMkjJRTb06weFhQBM5SwAnnwyxnhEREQySMlFtmzbBsAJvEBnPmflypjjERERyRAlF9nUty+GcwvXUF4edzAiIiKZoeQim95+m6204jz+zAU8EHc0IiIiGaHkIsvaTPg1n9OJa7kRPvww7nBEREQip+Qi2y6/nGu5gZ58DAccEHc0IiIikVNyEYNJXMQznBRedOkSbzAiIiIRU3IRizzO449hc+lSePrpeMMRERGJkJKLmKxgH7bntQ4vTj453mBEREQipOQiRq12bOFDeoQXJSXxBiMiIhIRJRcxePvtiu1+vM0y9mXrhm1w++3xBSUiIhIRJRcx6NsX3OHBB2EDJYzkYQrZzpM//SdDhsQdnYiISOMouYjRBReEJOPAUYP4K8P5Lk9R9NwMzODnP487OhERac6mTMlc3UoucsAf/gA/eGo0W2nFnzifDqzg5pvBDKZPjzs6ERFpjsaPz1zdSi5yxdChtO7UgXas5T16AzsAOP30kGR8/HGs0YmISDOT/H9lt92ir1vJRS5ZuhSAvVmBk09RUcVbPXuGWds3bowpNhERaVa2bw/rgQOjr1vJRa756KOdm5uP+TYbNkBe4rdUVgbFxbDnnjHFJiIizc7vfx99nUouck2PHnDooWH7+edpu3E55eXw+usVRdasCbdKnnoqlghFRKQZ6d07+jqVXOSi+fMrtvfZB4AjjwxPlkyYUPHWKafAUUdlNzQREZHaKLnIVRs2VGwfdNDOzcsvD0lGq1bh9RtvhL4YIiIiuULJRa5q2xaGDQvbixaFLKKSrVvhW98K22Vl4TbJv/6V5RhFRERSUHKRy/72t4renCnuf7z4IsyaVfH62GNh6NCsRCYiIk3Yp59mtn4lF7muvLxiu2PHXd4+5phwm6SgILx+5hlo3TpLsYmISJN0442ZrV/JRVMwdmxYr1qVdrzW7dvh8MPD9rZt4TbJ++9nKT4REWlSXnwxrPPzM1O/koum4NZbK3pw/vCHaYu9+Sb89a8Vr3v1ghEjMhybiIg0OcuWhXWbNpmpX8lFU7F1a8V2Df8ahg8Pt0nMwuvHHsvM0K4iItJ0bd4c1l/7WmbqV3LRlDz4YFhv2QJnnVVj0R074OCDw/bmzSHZyHQHHhERaRp2hOmrOPnkzNSv5KIpueCCirG/p04NGUNeHvzudymLv/tuRT4C0K0bXHhh5sMUEZGm4ZprMlOvkoumZvVq6Nq14rU7XHZZSDQKC3cZE/yCC0KGmrxN8uCDUFKSvXBFRCR3pXgIMRJKLpqizz4LScWKFdCuXcX+srIwJrhZ6GiReFzELCQY3buHYhs2hH1XXpn1yEVEpAVQctGUdewIX30VEo2FC6kyR/vmzeFxEbNwK2XlSj7+GB54oKLIhAnhrsrnn2c/dBERab6UXDQX3/hGSCjcYebMilG1IEyjutdeYMaPxnfDN25iwIDwlnu4y5Js1ZAUXnml4rktERGpVUHtRaTJOfnkMKoWwB13wE9/GrIICI+MtG3LbMCBz+nCPPqx8pOO/M3WMIgXaMe60KRRUBCG+2zfPmQgAwfCOedUjNbVEjz+OPzgB2H7s8+q9ncREWmC/vnPzB/DPPmfjlQxcOBAnz17dtxhROvyy+Huu3fZXUYe69iDAsrYnfV1r6vy/O/N0cqVocUnqVWrquONCABmNsfdB8YdRzrN8loWaYQTTwwJRl5e1RkmoryWdVukJfntb0MLRrWlwMtp76s5/KB15LOddnzF11nIXQU/Ca0WbdqEMWKTj5wk6zKDa6+N7+fJtE6dwnrvvaG4OIyrrvtHItLEzZsX1m3bZu4YSi5kpw8+gBWrClif147/4xtcVXYHtnoV9/92U3gSZceOkJCcdlrFh268MSQZt90WX+CZsN9+4WfOy4Mvvwx9Lszgk09qHIJdRCTXrVsX1gcdlLljKLmQKtq3D81kv/xlxb4f/zjcEUgOF8v06SHJ+Na3KgqNHRv+I648aldTdfbZUFoatpPr4uKKmX6mTIE//jGW0EREGqusLKzHjMncMZRcSEo33BDyh+Sdge3bw9AZxxxTqdCLL4ZCAxO36NzDEKB5eSEBaYoeeQQefjhs339/xQkA+OY3Ydy4sH3BBRWJh4hIEzRqVObqVnIhNVq6NPwfmuxu8dprYbvykjfnLVq3cm7I+yXrKAlJxumnhyTj+edjjb9eVq6EkSPD9mmnwUUX7Vrmv/+74mmZnj0rvgKIiMhOSi6kVl26hO4WF1+c+n330Ndx/I4b2IN1HMMsXuKb4M7Kb5/Jbfb/2N3W7nyqdb/9wrAcRxwB3/kOnHsu/PznoaHg1VdjfCAj2Uqxzz41t7y8+SbsvntozunZMzuxiYhEYI89snMcJRdSZ//7vykfNuFvf4MTTggDhhYUwOt2DIN4ie/zKEvoxk+5g3foy7nb7mfDV9soLQ2dR996C559FiZPhptvDn07jj8+DDRavXUkubRtm6FuHV26hFaI/Hz44ovay3/5ZQjos89gxIgMBCQiEq38/IrOnG+8kdljKbmQRhs2LDwzvWJF+DKffKjkcf8BAzfPgvbt6UIp9/NjNtOGz+nEk5zMGa3+TklJSCYKC8M//Ly8qk+8VrdpU+jWYRZaQSK56zJiRLj/AyFZqIuiIpg1K2w/9lhodhERyWHJadavvDK0HGeSkgvJrKIiWLWKgs0boV078tlBZ77gFJ7m8W2nsW69sXmLsW2vLpTNnk95eUVyUn3ZuhVOOikkIBCmVfn2t0Oi0aULvPdeA+KbPDkkBxCaRCp34KzN0UdXPFbz4x/Dxx83IAARkcxLfhErKIA778z88ZRcSHYUFVVMsrZlSxiiPD+/4v2lS6F//5AptGoVOmJU06oVPPNMeFR2/fpQvPLHe/cOHz/ooLrd2WDlyorjnH56eAKkvm64oeIRmoMOapodPJOZ26pVYXj4hQth9uzwNNCmTXFHJyIRuOaasN5nn+wcT3OLSPa1bh0mV0t65hk477xwXwXCvZXJk8MCYYTMxx+H447b+ZHiYpg7N2x/8UV468MPw+tFi0IDhFl4SvbVV0Nisot9961YT5vW8J9n1qww8+yaNdCtWzzTzLqHxGDuXHjnHfi//wsDfpWXwwEHwMaNYdmwIfV25TGAK1uwIGRtItKkLVgQ1g35DtUQmlskDc1HEBP30Adi+vTUrQB77RW+UR98cMqP//vf4dbJl19W3Z+XFxKOb34TfvITOPz0LqG5Iz8/mtaGLVtCb9MdO0IrSGOSlercQ+I1e3b4AT/4AJYsCfGvWhWacbZtS/3ZwsJwz6ht27AUF1ds1+X1EUeEfTXQ3CIiuS/Zl23r1jRftoj2WlZykYb+IOWIV1+FM8/cdcpzs/CIysyZaa+UJ54Idz3Wrq26/zDm8CrH8S69OZ5XKChuQ9euof/G1VfD177WwFjfequil9S998IllzSsnvJyuP12+P3vQ2tEums0Ly/cbtpjj9C607Ur7L9/SLz69YNDDw3vZ5iSC5HctmwZdO4ctmv6L1/JRRboD1IOWr0ajjwSFi+uur91a/jNb2ocy/b552HSJFj1zJs8vPIktlPIocxjGemnUM/Ph5KSMJTFgAFw4IGhEaBHj7DdsWOKD914Y8Vkbu+/D1//et1+ts2bw+ifDz8My5dXDaJLlzAwSM+eYX3ooXDYYWGsjRyg5EIkt3XtGu7WfuMboUtVOpFey+6uJcUyYMAAlxw2ZYp7mza7PlTStav7J5+k/szSpRXlfvCDnbunTXM/5RT3zp3dW7VyN0v1rErdl3P4o5dj/iwnemF+ubdp437MMe433ui+bFmleJYtcx850n333atW0KqV+1FHub/wQibPYGSA2Z4D12y6RdeytFQ7drj36VPxp2XjxprLR3ktq+UiDX3baUKGD6+YTC3JDM44o+IxUwjPYJWXh/bBOnS63L4d/vCH0JiwYkWoctOm0L1i27bwflkZKR+f/R+u5mp+zTj+m1sYt7PO3VnDCB7jPP7EUbxOa7axA8N2a4MNGhRml+3VK7pzkwVquRDJTWvWhL7mEO7S3ntvzeV1WyQL9AepCfr00/BYaPXEYbfdQsfGtWuj68BZm0odPLcPOJLFq/ak7Sfv0dk/p4BylrMX0xjONIbzAoPZRmsKCqBdu3An5TvfCVObdOlSv8Pu2BEOnUyC2rSBDh0y8hPupORCJPcsXQpXXBEetDvrrDCZc22UXGSB/iA1cb/9Lfz0p7s+RbFiRZrOEhkwd27orFHZPvvA+efDTTfx7/dbMWkSvPwyfPRReOgjOYJeZa1ahVYT911bSKC2DlphPJDKS9++tT4AUi9KLkRyT7t2FZ3Z16yp25wiUV7LGudCmqfLLw/Ltm1hwK6XXgpPX2QrsYDQ6fL3vw+jeI4bB1ddVWXgsL59dx0pb/PmcBtm2jR4++2QCxUVhRaIgoLw8eRSULDrUlhYdQ2hBeNvf6sYodwsdEhNJhv9+oX13ntn4ZyISMZ9/HFILNq0CXdZszVZWWVKLqR5a9UqTHwSlzFjanyKpbo2bcLtkFSzvTeGe7hbNG9exfLGG/DIIxVlOneuSDSSS48eNc/1IiK5J3kL5L33oHv3eGJQciHSApiFx9G6doXvfa9i/1dfwfz5YUkmHckh1iEM3TEwZ294iEgqDz8cpj6KK7EAJRciLdqee8LgwWFJ2rwZ3n03JBp9+sQXm4jUn3t2JiarjZILEamiTZvQWqEWC5GmJzl4cdw0K6qIiIhESsmFiIiIRKpFJBdm1tPM7jezx2ovLSJx0zUr0rTlfHJhZpPMbLmZLai2f6iZfWBmi83smprqcPeP3H1UZiMVEdA1KyJNo0Png8DdwB+TO8wsH/gdMAQoBd4ysxlAPnBztc9f5O7LEZFseRBdsyItWs4nF+7+spl1r7b7CGCxu38EYGZTgGHufjNwapZDFJFKdM2KSM7fFkmjC/BZpdeliX0pmVkHM7sX6G9m42ood7GZzTaz2StWrIguWhGJ/Jo1s+vMzGtbli5dGu1PIiK1yvmWizRSDUicdvomd18FXFpbpe5+H3AfwNChQzWjm0jdrKxDmcivWXe/DriutgPrWhaps7pcy3XSVJOLUmC/Sq+7ApF+PXnmmWeeMbPaZrnqSIS/jAjlalyQu7HlalyQu7HVJ66MX7Pp1PFajltnsnQ+GiGOGDN1zCjrbUxdDf1sfT5Xn7IHNCCWlJrElOuJ+7dPuHufxOsC4P+AbwOfA28BZ7v7u1mOa3YuTjWdq3FB7saWq3FB7sZWU1y5es3mKjNzd8/pKeLiiDFTx4yy3sbU1dDP1udzmSpbm5zvc2FmDwOvAV83s1IzG+XuZcBlwDPAQmCq/kiJ5AZdsyKS87dF3P2HafbPBGZmORwRqYWuWRHJ+ZaLHHdf3AGkkatxQe7GlqtxQe7GlqtxiUjMmkSfCxGR5kx9LrJ7TPW5iDaeVNRyISIiIpFSclEHtc2JYMGExPv/NrPDshDTfmb2gpktNLN3zezKFGUGmdlaM5ufWK7NdFyVjr3EzN5JHHd2ivfjOGdfr3Qu5pvZOjO7qlqZrJ2zVHNwmFl7M/uHmS1KrPdM89k6z9MRUVy/NrP3E7+raWbWLs1na/y9i0gL4e5aalgIcx98CPQEWgFvAwdXK3MK8BRhoKCjgDeyEFcn4LDEdgnhMb/qcQ0iPA4Yx3lbAnSs4f2sn7MUv9cvgG5xnTPgm8BhwIJK+24FrklsXwP8T5rYa/w3mYG4TgIKEtv/kyquuvzetaQ95x53DLkYY6aOGWW9jamroZ+tz+cyVba2RS0Xtds5J4K7bwOmAMOqlRkG/NGD14F2ZtYpk0G5+zJ3n5vYXk94vC/tcMo5KOvnrJpvAx+6+ydZPGYV7v4ysLra7mHAQ4nth4DTU3y0Lv8mI43L3Z/18DgpwOuEQbBERFJSclG7usyJUK95E6KWGLCoP/BGirePNrO3zewpM+udrZgIQzs/a2ZzzOziFO/Hes6AkcDDad6L65wB7OPuyyAkkMDeKcrEfe4uIrQ6pVLb711Suz7uAOogjhgzdcwo621MXQ39bH0+l6myNcr5cS5yQF3mRKjXvAlRMrNi4HHgKndfV+3tuYRm/w1mdgrwN+DAbMQFHOvuS81sb+AfZvZ+4htxUpznrBVwGpBqQqw4z1ldxXnu/gsoAyanKVLb711S8DBPSk6LI8ZMHTPKehtTV0M/W5/PZapsbdRyUbu6zIkQy7wJZlZISCwmu/tfq7/v7uvcfUNieyZQaFmaY8HdlybWy4FphKb8ymKbawI4GZjr7l9WfyPOc5bwZfL2UGK9PEWZuP69XUCYHv0cT9ygra4Ov3cRaQGUXNTuLeBAM+uR+MY7EphRrcwM4PzEExBHAWuTTduZYmYG3A8sdPc70pTZN1EOMzuC8Ptelcm4Esdqa2YlyW1CZ8AF1Ypl/ZxV8kPS3BKJ65xVMgO4ILF9ATA9RZm6/JuMlJkNBX4GnObum9KUqcvvXURaAN0WqYW7l5lZck6EfGCSu79rZpcm3r+XMKTxKcBiYBNwYRZCOxY4D3jHzOYn9v0c+FqluH4AjDGzMmAzMDLdN86I7QNMS/wfXQD8xd2fzoFzhpntBgwBLqm0r3JcWTtnFubgGAR0NLNSYDxwCzDVzEYBnwIjEmU7A39w91PS/ZvMcFzjgNaEWx0Ar7v7pZXjIs3vPaq4RKTp0AidIiLNQKK16GVgvLs/EXc8qcQRo85LNMysF3Al0BH4p7vfU1N53RYREakHq8MAdvWoa5cByyq9V9+B0n4GTE18tsjM3kw89fSumTX4KYD6xmhm+WY2z8xS/Se5M8Yoj1mLKsc0s3Zm9piFQeEWmtnRdagjqzFGLV2s9YnT3Re6+6XAmcDAWo+plgsRkbpLdLTt5O5zE31M5gCnu/t7lcrsDWxOjEGT3HeAuy+uVtc3gQ2EMV/6VNqfTxgYbwihA+9bhL5C+cDN1UK6COhL+EZZBKwEngTaJp56KgReBa5MjCmT6Rh/BJxDuDX2VrUYexK+1JYmv6Fn87y4+xNm9hDwirv/IdFnaTd3X5OF81LnGIlYqljrG6e7Lzez0wiD+93t7n+p6ZjqcyEiUg+JjsfJsUjWm1lyALv3KhX7FqHvzinuvsXMRgPDCf2MKtf1soVxaqrbOVAagJlNAYa5+82EJ3aqMLPBQFvgYEJfoZnJp56AwsRS/ZtkJmI8DxiQWP+nu++MNRFjP8KIvP8ys5nAqGyeFzN7hTAC7Y8Sx9kGbMvCealPjDPdfUeKehssTaz1ijNRzwxghpk9CSi5EBHJBEszgJ27P2pmPYApZvYo4RvqkHpUnWqgtCPTFXb3/0rE8yPCt98diW+mc4ADgN+5ezZiHA+cRZiSIF2MjxA6nv8womPW+bwAPYAVwANmdijh/Fzp7hsrfSb23109jtUY9YrTzAYBZxA6ds+srXL1uRARaQCreQA73P1WYAtwD+ER3g3Vy9RUfYp9td7DdvcHk83q7l7u7v0I46AcYWZ9UpSPMsZ+wCZ3n1NLjGcBn0R0TKjfeSkgzJtzj7v3BzYSmvmrl4/1d5cl9YrT3V909yvc/RJ3/11tlSu5kKwzs3KrOjtpZLN6mln3VB2sRKJktQxglyhzPNCHMJjY+HoeIrKB0hL9CV4EhmY4xgFANzNbQpjv5gQz+3OGj1nf81JK6O+RbMV5jJBs5FKM2ZLROJVcSBw2u3u/SsstcQckUldmdRrArj8wkTCh3IVAezO7qR6HadRAaWa2l5m1S2y3AU4E3s9wjHsCx7h790S8z7v7uRk+Zr3Oi7t/AXxmZl9P7Po2VfvKxB5jFmU0TiUXkjPMbImZ/Y+FR+jeNLMDEvu7mdk/zezfifXXEvv3MbNpFh63e9vMjklUlW9mEy08gvds4o+rSFSSA9idUKn17ZRqZXYDRrj7h4l76BcQbgVUYWHAsteAr5tZqYXB0/AwA21yoLSFwNR6DpTWCXjBzP5N+E/kHyma3OOIMe7zAnA5MDlxbvoB/52DMUYqVayZjlOPokrWmVk58E6lXTe7+yOJ5tSJ7v4rMzsfONPdTzWzvwOPuftDZnYR4R7o6YmOYa+5+52JzmvFhG9Pi4GB7j7fzKYCM9x9l+ZZERHJDCUXknVmtsHdi1PsXwKc4O4fJe5pf+HuHcxsJWFcge2J/cvcvaOZrQC6uvvWSnV0J3xLOzDx+mdAobvXp1lTREQaQbdFJNd4mu10ZVLZWmm7HD1yLSKSVUouJNecVWn9WmL7X4TORhBG/ns1sf1PYAzsHHJ492wFKSIi6ekbncShjVXM5ArwtLsnH0dtbWZvEBLfHyb2XQFMMrOxhAFwkjOoXgncl+hIVU5INLI1bbuIiKShPheSMxJ9Lga6+8q4YxERkYbTbRERERGJlFouREQkK9I9KSbNj1ouREQkNokxaqSZUXIhIiJZZWaDzOwFM/sLVQfUk2ZCT4uIiEgcjgD6uPvHcQci0VPLhYiIxOFNJRbNl5ILERGJw8a4A5DMUXIhIiIikVJyISIiIpHSOBciIiISKbVciIiISKSUXIiIiEiklFyIiIhIpJRciIiISKSUXIiIiEiklFyIiIhIpJRciIiISKSUXIiIiEik/j89QNktCVE2+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/eos/user/j/jthomasw/tdsm_encoder/datasets/ds2_diff_transforms/dataset_2_padded_nentry424To564.pt']\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "### HYPERPARAMETERS ###\n",
    "train_ratio = 0.8\n",
    "batch_size = 128\n",
    "n_epochs = 200\n",
    "epochs = tqdm.notebook.trange(n_epochs)\n",
    "# Setup exponentially decaying learning rate\n",
    "initial_lr = 1e-3\n",
    "\n",
    "### Model ###\n",
    "model = score_model.Gen(n_feat_dim, embed_dim, hidden_dim, num_encoder_blocks, num_attn_heads, dropout_gen, marginal_prob_std=marginal_prob_std_fn)\n",
    "state_dict = torch.load('initial_model.pt')\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "### optimiser ###\n",
    "optimiser = RAdam(model.parameters(),lr=initial_lr)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimiser, gamma=0.99)\n",
    "\n",
    "table = PrettyTable(['Module name', 'Parameters listed'])\n",
    "t_params = 0\n",
    "for name_ , para_ in model.named_parameters():\n",
    "    if not para_.requires_grad: continue\n",
    "    param = para_.numel()\n",
    "    table.add_row([name_, param])\n",
    "    t_params+=param\n",
    "print(table)\n",
    "print(f'Sum of trainable parameters: {t_params}')    \n",
    "\n",
    "output_directory = workingdir+'/training_'+datetime.now().strftime('%Y%m%d_%H%M')+'_'+preproc_dataset_name+'/'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "av_training_losses_per_epoch = []\n",
    "av_testing_losses_per_epoch = []\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(8,4))\n",
    "dh = display.display(fig, display_id=True)\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_yscale('log')\n",
    "ax[1].set_xlabel('lr')\n",
    "ax[1].set_xlim(initial_lr*0.99**(n_epochs), initial_lr)\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].tick_params('both', length=10, width=1, which='both')\n",
    "\n",
    "lrs_ = []\n",
    "\n",
    "print(files_list_)\n",
    "eps_ = []\n",
    "for epoch in epochs:\n",
    "    eps_.append(epoch)\n",
    "    # Create/clear per epoch variables\n",
    "    cumulative_epoch_loss = 0.\n",
    "    cumulative_test_epoch_loss = 0.\n",
    "\n",
    "    file_counter = 0\n",
    "    n_training_showers = 0\n",
    "    n_testing_showers = 0\n",
    "    training_batches_per_epoch = 0\n",
    "    testing_batches_per_epoch = 0\n",
    "\n",
    "    # Load files\n",
    "    for filename in files_list_:\n",
    "        custom_data = utils.cloud_dataset(filename, device=device)\n",
    "        train_size = int(train_ratio * len(custom_data.data))\n",
    "        test_size = len(custom_data.data) - train_size\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(custom_data, [train_size, test_size])\n",
    "        n_training_showers+=train_size\n",
    "        n_testing_showers+=test_size\n",
    "        \n",
    "        # Load clouds for each epoch of data dataloaders length will be the number of batches\n",
    "        shower_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        shower_loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Accumuate number of batches per epoch\n",
    "        training_batches_per_epoch += len(shower_loader_train)\n",
    "        testing_batches_per_epoch += len(shower_loader_test)\n",
    "\n",
    "        # Load shower batch for training\n",
    "        for i, (shower_data,incident_energies) in enumerate(shower_loader_train,0):\n",
    "            # Move model to device and set dtype as same as data (note torch.double works on both CPU and GPU)\n",
    "            model.to(device, shower_data.dtype)\n",
    "            model.train()\n",
    "            shower_data = shower_data.to(device)\n",
    "            incident_energies = incident_energies.to(device)\n",
    "\n",
    "            if len(shower_data) < 1:\n",
    "                print('Very few hits in shower: ', len(shower_data))\n",
    "                continue\n",
    "            # Zero any gradients from previous steps\n",
    "            optimiser.zero_grad()\n",
    "            # Loss average for each batch\n",
    "            loss = score_model.loss_fn(model, shower_data, incident_energies, marginal_prob_std_fn, padding_value, device=device)\n",
    "            # Accumulate batch loss per epoch\n",
    "            cumulative_epoch_loss+=float(loss)\n",
    "            # collect dL/dx for any parameters (x) which have requires_grad = True via: x.grad += dL/dx\n",
    "            loss.backward()\n",
    "            # Update value of x += -lr * x.grad\n",
    "            optimiser.step()\n",
    "\n",
    "        # Testing on subset of file\n",
    "        for i, (shower_data,incident_energies) in enumerate(shower_loader_test,0):\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                shower_data = shower_data.to(device)\n",
    "                incident_energies = incident_energies.to(device)\n",
    "                test_loss = score_model.loss_fn(model, shower_data, incident_energies, marginal_prob_std_fn, padding_value, device=device)\n",
    "                cumulative_test_epoch_loss+=float(test_loss)\n",
    "\n",
    "    # Calculate average loss per epoch\n",
    "    av_training_losses_per_epoch.append(cumulative_epoch_loss/training_batches_per_epoch)\n",
    "    av_testing_losses_per_epoch.append(cumulative_test_epoch_loss/testing_batches_per_epoch)\n",
    "    \n",
    "    lr_ = optimiser.param_groups[0]['lr']\n",
    "    epochs.set_description('Average Loss: {:5f}(Train) {:5f}(Test) {:5f}(lr)'.format(cumulative_epoch_loss/training_batches_per_epoch, cumulative_test_epoch_loss/testing_batches_per_epoch, lr_))\n",
    "    ax[0].plot(av_training_losses_per_epoch[1:], c='blue', label='training')\n",
    "    ax[0].plot(av_testing_losses_per_epoch[1:], c='red', label='testing')\n",
    "    \n",
    "    # End of epoch, change the learning rate\n",
    "    before_lr = optimiser.param_groups[0]['lr']\n",
    "    scheduler.step()\n",
    "    after_lr = optimiser.param_groups[0]['lr']\n",
    "    lrs_.append(before_lr)\n",
    "    ax[1].plot(lrs_[1:], av_training_losses_per_epoch[1:], c='blue')\n",
    "    if epoch == 0:\n",
    "        ax[0].legend(loc='upper right')\n",
    "    dh.update(fig)\n",
    "    if n_epochs%5 == 0:\n",
    "        torch.save(model.state_dict(), output_directory+'ckpt_tmp_'+str(epoch)+'.pth')\n",
    "    \n",
    "fig.savefig(output_directory+'loss_v_epoch.png')\n",
    "torch.save(model.state_dict(), output_directory+'ckpt_tmp_'+str(epoch)+'.pth')\n",
    "\n",
    "util.display.plot_loss_vs_epoch(eps_, av_training_losses_per_epoch, av_testing_losses_per_epoch, odir=output_directory, zoom=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4a39c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
