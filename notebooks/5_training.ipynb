{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd482aa1",
   "metadata": {},
   "source": [
    "# Training\n",
    "Here we provide interactive scripts, with which one can train a generative model. The script uses classes and functions used in the code that runs on condor in order to obtain the optimised model trained on the full dataset. \n",
    "We advise not to train the model on the full dataset here as it will take too long. Limit the training dataset size (e.g. use a single input file) and number of epochs and use this notebook to study e.g pre-processing, hyperparameters and diffusion techniques.\n",
    "\n",
    "First, we set the environment and the various parameters for the model, hyperparameters, diffusion equations etc. We also create instances of the stochastic differential equations, machine learning model and create a list of files we want to use as input.\n",
    "\n",
    "Important: need to set the padding value to the value used in pad_events.py script to create the padded (and transformed) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988fc9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, functools, torch, os,sys, random, fnmatch, psutil\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam,RAdam\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from prettytable import PrettyTable\n",
    "import tqdm\n",
    "from pickle import load\n",
    "from IPython import display\n",
    "\n",
    "sys.path.insert(1, '../')\n",
    "import trans_tdsm, utils\n",
    "import util.dataset_structure, util.display, util.model\n",
    "\n",
    "# GPU device info\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:516\"\n",
    "os.system('nvidia-smi')\n",
    "\n",
    "# Set padding value used\n",
    "padding_value = 0.0\n",
    "\n",
    "dataset = \"dataset_2_padded_nentry\"\n",
    "preproc_dataset_name = 'ds2_diff_transforms'\n",
    "dataset_store_path = os.path.join(\"/eos/user/j/jthomasw/tdsm_encoder/datasets/\", preproc_dataset_name)\n",
    "transform = None\n",
    "transform_y = None\n",
    "mask = True\n",
    "jupyternotebook = True\n",
    "workingdir = \"./\"\n",
    "\n",
    "### SDE PARAMETERS ###\n",
    "SDE = 'VP'\n",
    "if SDE == 'VP':\n",
    "    beta_max = 1.0\n",
    "    beta_min = 0.001\n",
    "if SDE == 'VE':\n",
    "    sigma_max = 20.0\n",
    "    sigma_min = 0.1\n",
    "    \n",
    "### MODEL PARAMETERS ###\n",
    "n_feat_dim = 4\n",
    "embed_dim = 512\n",
    "hidden_dim = 128\n",
    "num_encoder_blocks = 8\n",
    "num_attn_heads = 16\n",
    "dropout_gen = 0\n",
    "\n",
    "# Instantiate stochastic differential equation\n",
    "if SDE == 'VP':\n",
    "    sde = utils.VPSDE(beta_max=beta_max,beta_min=beta_min, device=device)\n",
    "if SDE == 'VE':\n",
    "    sde = utils.VESDE(sigma_max=sigma_max,sigma_min=sigma_min,device=device)\n",
    "marginal_prob_std_fn = functools.partial(sde.marginal_prob)\n",
    "diffusion_coeff_fn = functools.partial(sde.sde)\n",
    "\n",
    "print('torch version: ', torch.__version__)\n",
    "print('Running on device: ', device)\n",
    "if torch.cuda.is_available():\n",
    "    print('Cuda used to build pyTorch: ',torch.version.cuda)\n",
    "    print('Current device: ', torch.cuda.current_device())\n",
    "    print('Cuda arch list: ', torch.cuda.get_arch_list())\n",
    "\n",
    "print('Working directory: ', workingdir)\n",
    "\n",
    "# Input files\n",
    "files_list_ = []\n",
    "for filename in os.listdir(dataset_store_path):\n",
    "    if fnmatch.fnmatch(filename, dataset + '*424To564.pt'):\n",
    "        files_list_.append(os.path.join(dataset_store_path, filename))\n",
    "print(files_list_)\n",
    "\n",
    "# Instantiate model\n",
    "model=trans_tdsm.Gen(n_feat_dim, embed_dim, hidden_dim, num_encoder_blocks, num_attn_heads, dropout_gen, marginal_prob_std=marginal_prob_std_fn)\n",
    "torch.save(model.state_dict(), 'initial_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780fe119",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The following cell will call training methods from the trans_tdsm.py script so everything should be synchronised with what we can run on condor for the big jobs.\n",
    "\n",
    "Once you have a a fully trained model you can look at the sampling notebook to see how to generate samples and make some plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa3b437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "### HYPERPARAMETERS ###\n",
    "train_ratio = 0.9\n",
    "batch_size = 128\n",
    "n_epochs = 100\n",
    "epochs = tqdm.notebook.trange(n_epochs)\n",
    "# Setup exponentially decaying learning rate\n",
    "initial_lr = 1e-3\n",
    "\n",
    "### Model ###\n",
    "model = trans_tdsm.Gen(n_feat_dim, embed_dim, hidden_dim, num_encoder_blocks, num_attn_heads, dropout_gen, marginal_prob_std=marginal_prob_std_fn)\n",
    "state_dict = torch.load('initial_model.pt')\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "### optimiser ###\n",
    "optimiser = RAdam(model.parameters(),lr=initial_lr)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimiser, gamma=0.99)\n",
    "\n",
    "table = PrettyTable(['Module name', 'Parameters listed'])\n",
    "t_params = 0\n",
    "for name_ , para_ in model.named_parameters():\n",
    "    if not para_.requires_grad: continue\n",
    "    param = para_.numel()\n",
    "    table.add_row([name_, param])\n",
    "    t_params+=param\n",
    "print(table)\n",
    "print(f'Sum of trainable parameters: {t_params}')    \n",
    "\n",
    "output_directory = workingdir+'/training_'+datetime.now().strftime('%Y%m%d_%H%M')+'_'+preproc_dataset_name+'/'\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)\n",
    "    \n",
    "av_training_losses_per_epoch = []\n",
    "av_testing_losses_per_epoch = []\n",
    "\n",
    "fig, ax = plt.subplots(ncols=2, figsize=(8,4))\n",
    "dh = display.display(fig, display_id=True)\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_yscale('log')\n",
    "ax[1].set_xlabel('lr')\n",
    "ax[1].set_xlim(initial_lr*0.99**(n_epochs), initial_lr)\n",
    "ax[1].set_xscale('log')\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].tick_params('both', length=10, width=1, which='both')\n",
    "\n",
    "lrs_ = []\n",
    "\n",
    "print(files_list_)\n",
    "eps_ = []\n",
    "for epoch in epochs:\n",
    "    eps_.append(epoch)\n",
    "    # Create/clear per epoch variables\n",
    "    cumulative_epoch_loss = 0.\n",
    "    cumulative_test_epoch_loss = 0.\n",
    "\n",
    "    file_counter = 0\n",
    "    n_training_showers = 0\n",
    "    n_testing_showers = 0\n",
    "    training_batches_per_epoch = 0\n",
    "    testing_batches_per_epoch = 0\n",
    "\n",
    "    # Load files\n",
    "    for filename in files_list_:\n",
    "        custom_data = utils.cloud_dataset(filename, device=device)\n",
    "        train_size = int(train_ratio * len(custom_data.data))\n",
    "        test_size = len(custom_data.data) - train_size\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(custom_data, [train_size, test_size])\n",
    "        n_training_showers+=train_size\n",
    "        n_testing_showers+=test_size\n",
    "        \n",
    "        # Load clouds for each epoch of data dataloaders length will be the number of batches\n",
    "        shower_loader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        shower_loader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Accumuate number of batches per epoch\n",
    "        training_batches_per_epoch += len(shower_loader_train)\n",
    "        testing_batches_per_epoch += len(shower_loader_test)\n",
    "\n",
    "        # Load shower batch for training\n",
    "        for i, (shower_data,incident_energies) in enumerate(shower_loader_train,0):\n",
    "            # Move model to device and set dtype as same as data (note torch.double works on both CPU and GPU)\n",
    "            model.to(device, shower_data.dtype)\n",
    "            model.train()\n",
    "            shower_data = shower_data.to(device)\n",
    "            incident_energies = incident_energies.to(device)\n",
    "\n",
    "            if len(shower_data) < 1:\n",
    "                print('Very few hits in shower: ', len(shower_data))\n",
    "                continue\n",
    "            # Zero any gradients from previous steps\n",
    "            optimiser.zero_grad()\n",
    "            # Loss average for each batch\n",
    "            loss = trans_tdsm.loss_fn(model, shower_data, incident_energies, marginal_prob_std_fn, padding_value, device=device)\n",
    "            # Accumulate batch loss per epoch\n",
    "            cumulative_epoch_loss+=float(loss)\n",
    "            # collect dL/dx for any parameters (x) which have requires_grad = True via: x.grad += dL/dx\n",
    "            loss.backward()\n",
    "            # Update value of x += -lr * x.grad\n",
    "            optimiser.step()\n",
    "\n",
    "        # Testing on subset of file\n",
    "        for i, (shower_data,incident_energies) in enumerate(shower_loader_test,0):\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                shower_data = shower_data.to(device)\n",
    "                incident_energies = incident_energies.to(device)\n",
    "                test_loss = trans_tdsm.loss_fn(model, shower_data, incident_energies, marginal_prob_std_fn, padding_value, device=device)\n",
    "                cumulative_test_epoch_loss+=float(test_loss)\n",
    "\n",
    "    # Calculate average loss per epoch\n",
    "    av_training_losses_per_epoch.append(cumulative_epoch_loss/training_batches_per_epoch)\n",
    "    av_testing_losses_per_epoch.append(cumulative_test_epoch_loss/testing_batches_per_epoch)\n",
    "    \n",
    "    lr_ = optimiser.param_groups[0]['lr']\n",
    "    epochs.set_description('Average Loss: {:5f}(Train) {:5f}(Test) {:5f}(lr)'.format(cumulative_epoch_loss/training_batches_per_epoch, cumulative_test_epoch_loss/testing_batches_per_epoch, lr_))\n",
    "    ax[0].plot(av_training_losses_per_epoch[1:], c='blue', label='training')\n",
    "    ax[0].plot(av_testing_losses_per_epoch[1:], c='red', label='testing')\n",
    "    \n",
    "    # End of epoch, change the learning rate\n",
    "    before_lr = optimiser.param_groups[0]['lr']\n",
    "    scheduler.step()\n",
    "    after_lr = optimiser.param_groups[0]['lr']\n",
    "    lrs_.append(before_lr)\n",
    "    ax[1].plot(lrs_[1:], av_training_losses_per_epoch[1:], c='blue')\n",
    "    if epoch == 0:\n",
    "        ax[0].legend(loc='upper right')\n",
    "    dh.update(fig)\n",
    "    if n_epochs%5 == 0:\n",
    "        torch.save(model.state_dict(), output_directory+'ckpt_tmp_'+str(epoch)+'.pth')\n",
    "    \n",
    "fig.savefig(output_directory+'loss_v_epoch.png')\n",
    "torch.save(model.state_dict(), output_directory+'ckpt_tmp_'+str(epoch)+'.pth')\n",
    "\n",
    "util.display.plot_loss_vs_epoch(eps_, av_training_losses_per_epoch, av_testing_losses_per_epoch, odir=output_directory, zoom=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c154c517",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
